This project focuses on the application of deep learning techniques for Human Action Recognition (HAR) using video data. The primary objective is to classify human actions by analyzing both the spatial and temporal features present in sequences of video frames. To achieve this, we implemented and compared two advanced models: ConvLSTM (Convolutional Long Short-Term Memory) and LRCN (Long-term Recurrent Convolutional Networks). ConvLSTM combines convolutional operations with LSTM units to process spatial and temporal patterns simultaneously, whereas LRCN follows a two-stage architecture where CNNs are used for spatial feature extraction and LSTMs for temporal sequence modeling. Both models are designed to understand the dynamics of human motion across time and space. We used a subset of the UCF50 dataset for training and evaluation, selecting specific action classes such as WalkingWithDog, TaiChi, Swing, HorseRace, JumpingJack, and PlayingGuitar. Video data was preprocessed by extracting frames and organizing them into time-based sequences labeled with the corresponding actions. Each model was trained under the same conditions, and their performance was evaluated using metrics such as accuracy, loss, and confusion matrix analysis. The ConvLSTM model demonstrated slightly better accuracy, benefiting from its integrated approach to spatio-temporal learning, while LRCN also produced strong results with clear recognition of motion patterns over time. Both models were successful in recognizing actions from unseen videos, highlighting the effectiveness of deep learning in video classification tasks. The results underscore the importance of temporal modeling in accurately identifying human actions. This study confirms that combining convolutional and recurrent neural networks provides a robust framework for recognizing complex human behaviors in video data. Such methods have potential applications in fields like surveillance, healthcare, human-computer interaction, and sports analytics. The project contributes to the growing body of research in video-based activity recognition and emphasizes the role of temporal features in deep learning models.
